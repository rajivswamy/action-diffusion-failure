{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c38acc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Imports**\n",
    "# diffusion policy import\n",
    "from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import zarr\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from skvideo.io import vwrite\n",
    "from IPython.display import Video\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import glob\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd53efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General failure detector interface --> ideally implement this downstream\n",
    "class FailureDetector(ABC):\n",
    "    def __init__(self, calibration_dataset_dir: str, config: Dict):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def detect_episode(self, episode_dataset_dir) -> bool:\n",
    "        \"\"\"\n",
    "        Detects if a failure has occurred in the environment.\n",
    "        :param obs: The observation from the environment.\n",
    "        :return: True, detection time if a failure is detected, False, None otherwise.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def calibrate(self):\n",
    "        \"\"\"\n",
    "        Calibrates the failure detector.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74eb4c",
   "metadata": {},
   "source": [
    "## Episode Dataset Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973efa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union, List, Generator, Optional\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import IterableDataset\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def load_pickle(path: Union[str, pathlib.Path]) -> pd.DataFrame:\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "class EpisodeDataset(IterableDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: Union[str, pathlib.Path],\n",
    "        exec_horizon: int,\n",
    "        sample_history: int,\n",
    "        filter_success: bool = False,\n",
    "        filter_failure: bool = False,\n",
    "        filter_episodes: Optional[List[int]] = None,\n",
    "        max_episode_length: Optional[int] = None,\n",
    "        max_num_episodes: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Construct EpisodeDataset.\"\"\"\n",
    "        super().__init__()\n",
    "        assert exec_horizon >= 1 and sample_history >= 0\n",
    "        self._dataset_path = dataset_path\n",
    "        self._episode_files = sorted(glob.glob(os.path.join(dataset_path, \"*.pkl\")))\n",
    "        self._exec_horizon = exec_horizon\n",
    "        self._sample_history = sample_history\n",
    "        self._filter_success = filter_success\n",
    "        self._filter_failure = filter_failure\n",
    "        self._filter_episodes = filter_episodes\n",
    "        self._max_episode_length = max_episode_length\n",
    "        self._max_num_episodes = max_num_episodes\n",
    "\n",
    "    def __iter__(\n",
    "        self,\n",
    "    ) -> Generator[Union[Dict[str, Any], List[Dict[str, Any]]], None, None]:\n",
    "        \"\"\"Return sample.\"\"\"\n",
    "        num_episodes = 0\n",
    "        for i, file_path in enumerate(self._episode_files):\n",
    "            # if self._max_num_episodes is not None and num_episodes >= self._max_num_episodes:\n",
    "            if self._max_num_episodes is not None and i >= self._max_num_episodes:\n",
    "                continue\n",
    "\n",
    "            episode = load_pickle(file_path)\n",
    "            success = episode.iloc[0].to_dict().get(\"success\", True)\n",
    "            if (\n",
    "                (self._filter_success and success)\n",
    "                or (self._filter_failure and not success)\n",
    "                or (\n",
    "                    self._filter_episodes is not None\n",
    "                    and not isinstance(self._filter_episodes, str)\n",
    "                    and i in self._filter_episodes\n",
    "                )\n",
    "            ):\n",
    "                continue\n",
    "            else:\n",
    "                num_episodes += 1\n",
    "\n",
    "            for idx in range(\n",
    "                self._exec_horizon * self._sample_history,\n",
    "                len(episode), # rows of data in episode df\n",
    "                self._exec_horizon,\n",
    "            ):\n",
    "                if (\n",
    "                    self._max_episode_length is not None\n",
    "                    and episode.iloc[idx].to_dict()[\"timestep\"]\n",
    "                    >= self._max_episode_length\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                sample = [\n",
    "                    episode.iloc[j].to_dict()\n",
    "                    for j in range(\n",
    "                        idx - self._exec_horizon * self._sample_history,\n",
    "                        idx + 1,\n",
    "                        self._exec_horizon,\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "                # if len(sample) < 2:\n",
    "                #     continue\n",
    "                # assert all(x[\"episode\"] == i for x in sample), not relevant for our dataset\n",
    "                yield sample[0] if len(sample) == 1 else sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f98acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = EpisodeDataset(\n",
    "    \"logs/datasets/no_domain_randomization_v5\",\n",
    "    exec_horizon=1,\n",
    "    sample_history=0,\n",
    "    filter_success=False,\n",
    "    filter_failure=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87378503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed iteration over dataset: 8954 samples seen.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "try:\n",
    "    for sample in test_dataset:\n",
    "        # Optional sanity checks on each sample:\n",
    "        # assert isinstance(sample, dict) or list\n",
    "        # if list: assert all(\"timestep\" in s for s in sample)\n",
    "        count += 1\n",
    "    print(f\"✅ Completed iteration over dataset: {count} samples seen.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error after {count} samples: {e!r}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a68c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(test_dataset)\n",
    "\n",
    "sample = next(it) # does return the dict with just "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7b1ee78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['timestep', 'rgb', 'reward', 'sampled_actions', 'executed_action', 'action_index', 'agent_positions', 'agent_velocities', 'block_poses', 'goal_poses', 'step_image_features', 'success', 'episode'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d7ee007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 512)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['step_image_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64c84935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([192., 375.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['agent_positions'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85e492",
   "metadata": {},
   "source": [
    "### Testing STAC Pipeline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a7f99",
   "metadata": {},
   "source": [
    "error_fns in eval script:\n",
    "- \"mmd_rbf_all_median\"\n",
    "- \"kde_kl_all_for_eig\"\n",
    "- \"kde_kl_all_rev_eig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4b9c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Dict, Callable, Any, List, Optional\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import hydra\n",
    "import random\n",
    "import pickle\n",
    "import imageio\n",
    "import pathlib\n",
    "import omegaconf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.stac import utils\n",
    "import src.stac.dataset_utils as data_utils\n",
    "from src.stac import error_utils, metric_utils, action_utils\n",
    "\n",
    "import omegaconf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8e260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSISTENCY_AGGR_FNS: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    \"min\": np.min,\n",
    "    \"max\": np.max,\n",
    "    \"mean\": np.mean,\n",
    "    \"std_dev\": np.std,\n",
    "    \"var\": np.var,\n",
    "}\n",
    "\n",
    "CONSISTENCY_ERROR_FNS: Dict[str, Dict[str, Any]] = {\n",
    "    \"mse_all\": {\n",
    "        \"error_fn\": \"mse\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "    },\n",
    "    \"mse_pos\": {\n",
    "        \"error_fn\": \"mse\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": True,\n",
    "    },\n",
    "    \"ate_pos\": {\n",
    "        \"error_fn\": \"ate\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "CONSISTENCY_DIST_ERROR_FNS = {\n",
    "    # MMD. - Maximum Mean Discrepancy (MMD)\n",
    "    \"mmd_rbf_pos\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": True,\n",
    "        \"gamma\": 1.0,\n",
    "    },\n",
    "    \"mmd_rbf_all\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": None,\n",
    "    },\n",
    "    \"mmd_rbf_all_1.0\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": 1.0,\n",
    "    },\n",
    "    \"mmd_rbf_all_median\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": \"median\",\n",
    "    },\n",
    "    \"mmd_rbf_all_eig\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": \"max_eig\",\n",
    "    },\n",
    "    \"mmd_rbf_all_0.1\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": 0.1,\n",
    "    },\n",
    "    \"mmd_rbf_all_0.5\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": 0.5,\n",
    "    },\n",
    "    \"mmd_rbf_all_5.0\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": 5.0,\n",
    "    },\n",
    "    \"mmd_rbf_all_10.0\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": 10.0,\n",
    "    },\n",
    "    \"mmd_rbf_all_100.0\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": 100.0,\n",
    "    },\n",
    "    # KDE For. KL.\n",
    "    \"kde_kl_all_for\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": 1.0,\n",
    "    },\n",
    "    \"kde_kl_all_for_eig\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": \"max_eig\",\n",
    "    },\n",
    "    \"kde_kl_all_for_0.1\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": 0.1,\n",
    "    },\n",
    "    \"kde_kl_all_for_0.5\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": 0.5,\n",
    "    },\n",
    "    \"kde_kl_all_for_5.0\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": 5.0,\n",
    "    },\n",
    "    \"kde_kl_all_for_10.0\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": 10.0,\n",
    "    },\n",
    "    \"kde_kl_all_for_100.0\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": 100.0,\n",
    "    },\n",
    "    # KDE Rev. KL.\n",
    "    \"kde_kl_all_rev\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": 1.0,\n",
    "    },\n",
    "    \"kde_kl_all_rev_eig\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": \"max_eig\",\n",
    "    },\n",
    "    \"kde_kl_all_rev_0.1\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": 0.1,\n",
    "    },\n",
    "    \"kde_kl_all_rev_0.5\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": 0.5,\n",
    "    },\n",
    "    \"kde_kl_all_rev_5.0\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": 5.0,\n",
    "    },\n",
    "    \"kde_kl_all_rev_10.0\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": 10.0,\n",
    "    },\n",
    "    \"kde_kl_all_rev_100.0\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": 100.0,\n",
    "    },\n",
    "    \"wasserstein\": {\n",
    "        \"error_fn\": \"wass\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": None,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Experiment keys.\n",
    "def temporal_consistency_exp_key(\n",
    "    pred_horizon: int,\n",
    "    sample_size: int,\n",
    "    error_fn: str,\n",
    "    aggr_fn: Optional[str] = None,\n",
    ") -> str:\n",
    "    if error_fn in CONSISTENCY_DIST_ERROR_FNS:\n",
    "        return (\n",
    "            f\"pred_horizon_{pred_horizon}_sample_size_{sample_size}_error_fn_{error_fn}\"\n",
    "        )\n",
    "    else:\n",
    "        return f\"pred_horizon_{pred_horizon}_sample_size_{sample_size}_error_fn_{error_fn}_aggr_fn_{aggr_fn}\"\n",
    "    \n",
    "def quantile_exp_key(exp_key: str, quantile: float = 0.95) -> str:\n",
    "    return f\"{exp_key}_quantile_{quantile}\"\n",
    "\n",
    "def get_consistency_aggr_fns(\n",
    "    cfg: omegaconf.DictConfig,\n",
    "    error_fn: str,\n",
    ") -> List[Optional[str]]:\n",
    "    if error_fn not in CONSISTENCY_ERROR_FNS:\n",
    "        return [None]\n",
    "    return cfg.eval.consistency.aggr_fns\n",
    "\n",
    "def compute_cum_scores(\n",
    "    results_frame: pd.DataFrame,\n",
    "    exp_keys: List[str],\n",
    ") -> pd.DataFrame:\n",
    "    for exp_key in exp_keys:\n",
    "        cum_scores = pd.Series(\n",
    "            data_utils.aggr_episode_key_data(\n",
    "                results_frame,\n",
    "                f\"{exp_key}_score\",\n",
    "                np.cumsum,\n",
    "            ),\n",
    "            name=f\"{exp_key}_cum_score\",\n",
    "        )\n",
    "        results_frame = pd.concat([results_frame, cum_scores], axis=1)\n",
    "\n",
    "    return results_frame\n",
    "\n",
    "# get RGB image from data\n",
    "def get_rgb(data: Dict[str, Any]) -> Optional[np.ndarray]:\n",
    "    return data['rgb'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3404cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temporal_consistency_errors(\n",
    "    cfg: omegaconf.DictConfig, dataset: EpisodeDataset\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute temporal consistency errors over dataset.\"\"\"\n",
    "    results = []\n",
    "    exp_keys = []\n",
    "\n",
    "    for prev_data, curr_data in iter(dataset):\n",
    "        assert curr_data[\"timestep\"] - prev_data[\"timestep\"] == cfg.model.ac_horizon\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"episode\": curr_data[\"episode\"],\n",
    "                \"timestep\": curr_data[\"timestep\"],\n",
    "                \"success\": curr_data.get(\"success\", True),\n",
    "            }\n",
    "        )\n",
    "        rgb = get_rgb(curr_data)\n",
    "        if isinstance(rgb, np.ndarray):\n",
    "            results[-1][\"rgb\"] = rgb\n",
    "\n",
    "        for sample_size in cfg.eval.consistency.sample_sizes:\n",
    "\n",
    "            # Subsample current and previous actions.\n",
    "            curr_actions = action_utils.subsample_actions(\n",
    "                curr_data[\"sampled_actions\"],\n",
    "                sample_size,\n",
    "            )\n",
    "            curr_skip_steps = curr_data.get(\"skip_steps\", None) # Should be None\n",
    "\n",
    "            prev_actions = action_utils.subsample_actions(\n",
    "                prev_data[\"sampled_actions\"],\n",
    "                sample_size,\n",
    "            )\n",
    "            prev_skip_steps = prev_data.get(\"skip_steps\", None) # Should be None\n",
    "\n",
    "            for error_fn in cfg.eval.consistency.error_fns:\n",
    "\n",
    "                if error_fn in CONSISTENCY_ERROR_FNS:\n",
    "                    prev_selected_actions = prev_data[\"executed_action\"]\n",
    "                    error_fn_kwargs = CONSISTENCY_ERROR_FNS[error_fn]\n",
    "                elif error_fn in CONSISTENCY_DIST_ERROR_FNS:\n",
    "                    prev_selected_actions = prev_actions\n",
    "                    error_fn_kwargs = CONSISTENCY_DIST_ERROR_FNS[error_fn]\n",
    "                else:\n",
    "                    raise ValueError(f\"Error function {error_fn} not supported.\")\n",
    "\n",
    "                for aggr_fn in get_consistency_aggr_fns(cfg, error_fn):\n",
    "                    for pred_horizon in cfg.eval.consistency.pred_horizons:\n",
    "                        if cfg.model.ac_horizon >= pred_horizon:\n",
    "                            continue\n",
    "\n",
    "                        exp_key = temporal_consistency_exp_key(\n",
    "                            pred_horizon=pred_horizon,\n",
    "                            sample_size=sample_size,\n",
    "                            error_fn=error_fn,\n",
    "                            aggr_fn=aggr_fn,\n",
    "                        )\n",
    "                        if exp_key not in exp_keys:\n",
    "                            exp_keys.append(exp_key)\n",
    "\n",
    "                        error = error_utils.compute_temporal_error(\n",
    "                            curr_action=curr_actions,\n",
    "                            prev_action=prev_selected_actions,\n",
    "                            pred_horizon=pred_horizon,\n",
    "                            exec_horizon=cfg.model.ac_horizon,\n",
    "                            sim_freq=cfg.env.args.freq,\n",
    "                            num_robots=cfg.env.num_eef,\n",
    "                            action_dim=cfg.env.dof,\n",
    "                            skip_steps=False,\n",
    "                            curr_skip_steps=curr_skip_steps,\n",
    "                            prev_skip_steps=prev_skip_steps,\n",
    "                            **error_fn_kwargs,\n",
    "                        )\n",
    "                        if error_fn in CONSISTENCY_ERROR_FNS:\n",
    "                            error = CONSISTENCY_AGGR_FNS[aggr_fn](error)\n",
    "                        results[-1][f\"{exp_key}_score\"] = error\n",
    "\n",
    "    results_frame = compute_cum_scores(pd.DataFrame(results), exp_keys)\n",
    "    return results_frame\n",
    "\n",
    "\n",
    "def evaluate_temporal_consistency(\n",
    "    cfg: omegaconf.DictConfig,\n",
    "    demo_dataset_path: Union[str, pathlib.Path],\n",
    "    test_dataset_path: Union[str, pathlib.Path],\n",
    ") -> Dict[str, Union[Dict[str, Any], pd.DataFrame]]:\n",
    "    \"\"\"Compute temporal consistency results.\"\"\"\n",
    "    # Construct episode iterable datasets.\n",
    "    demo_dataset = EpisodeDataset(\n",
    "        dataset_path=demo_dataset_path,\n",
    "        exec_horizon=1,\n",
    "        sample_history=1,\n",
    "        filter_success=getattr(cfg.eval, \"filter_demo_success\", False),\n",
    "        filter_failure=getattr(cfg.eval, \"filter_demo_failure\", True),\n",
    "        filter_episodes=getattr(cfg.eval, \"filter_demo_episodes\", None),\n",
    "        max_episode_length=getattr(cfg.eval, \"max_demo_episode_length\", None),\n",
    "        max_num_episodes=getattr(cfg.eval, \"max_num_episodes\", None),\n",
    "    )\n",
    "    test_dataset = EpisodeDataset(\n",
    "        dataset_path=test_dataset_path,\n",
    "        exec_horizon=1,\n",
    "        sample_history=1,\n",
    "        filter_success=getattr(cfg.eval, \"filter_test_success\", False),\n",
    "        filter_failure=getattr(cfg.eval, \"filter_test_failure\", False),\n",
    "        filter_episodes=getattr(cfg.eval, \"filter_test_episodes\", None),\n",
    "        max_episode_length=getattr(cfg.eval, \"max_test_episode_length\", None),\n",
    "        max_num_episodes=getattr(cfg.eval, \"max_num_episodes\", None),\n",
    "    )\n",
    "\n",
    "    # Compute scores for specified parameter sets.\n",
    "    results_dict = defaultdict(dict)\n",
    "    demo_results_frame = compute_temporal_consistency_errors(cfg, demo_dataset)\n",
    "    test_results_frame = compute_temporal_consistency_errors(cfg, test_dataset)\n",
    "\n",
    "    # Compute metrics for specified parameter sets.\n",
    "    for sample_size in cfg.eval.consistency.sample_sizes:\n",
    "        for error_fn in cfg.eval.consistency.error_fns:\n",
    "            for aggr_fn in get_consistency_aggr_fns(cfg, error_fn):\n",
    "                for pred_horizon in cfg.eval.consistency.pred_horizons:\n",
    "                    if cfg.model.ac_horizon >= pred_horizon:\n",
    "                        continue\n",
    "\n",
    "                    exp_key = temporal_consistency_exp_key(\n",
    "                        pred_horizon=pred_horizon,\n",
    "                        sample_size=sample_size,\n",
    "                        error_fn=error_fn,\n",
    "                        aggr_fn=aggr_fn,\n",
    "                    )\n",
    "\n",
    "                    for quantile in cfg.eval.quantiles:\n",
    "\n",
    "                        test_results_frame = metric_utils.compute_detection_results(\n",
    "                            exp_key=exp_key,\n",
    "                            quantile_key=quantile_exp_key(exp_key, quantile),\n",
    "                            results_dict=results_dict,\n",
    "                            demo_results_frame=demo_results_frame,\n",
    "                            test_results_frame=test_results_frame,\n",
    "                            detector=getattr(cfg.eval, \"detector\", \"quantile\"),\n",
    "                            detector_kwargs={\n",
    "                                \"quantile\": quantile,\n",
    "                                **getattr(cfg.eval, \"detector_kwargs\", {}),\n",
    "                            },\n",
    "                        )\n",
    "\n",
    "    return {\n",
    "        \"results_dict\": results_dict,\n",
    "        \"test_results_frame\": test_results_frame,\n",
    "        \"demo_results_frame\": demo_results_frame,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3d6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dict = {\n",
    "    \"env\": {\n",
    "        \"args\": {\n",
    "            \"freq\": 1,\n",
    "            \"max_episode_length\": 300\n",
    "        },\n",
    "        \"dof\": 2,\n",
    "        \"num_eef\": 1\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"ac_horizon\": 8,\n",
    "    },\n",
    "    \"eval\": {\n",
    "        \"consistency\": {\n",
    "            \"sample_sizes\": [128, 256],\n",
    "            \"error_fns\": [\"mse_all\", \"mmd_rbf_all_median\", \"kde_kl_all_for_eig\"], \n",
    "            \"pred_horizons\": [16],\n",
    "            \"aggr_fns\": [\"min\"]\n",
    "        },\n",
    "        \"quantiles\": [0.95],\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg = omegaconf.OmegaConf.create(conf_dict)\n",
    "\n",
    "dataset_path = \"logs/datasets/no_domain_randomization_v5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be784a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Results: ep_iid_cum | pred_horizon_16_sample_size_128_error_fn_mse_all_aggr_fn_min_quantile_0.95\n",
      "TPR: 0.46 | TNR: 0.65 | Acc: 0.55 | Bal. Acc: 0.55\n",
      "TP Time 108.78 (87.47)\n",
      "\n",
      "Episode Results: ep_iid_cum | pred_horizon_16_sample_size_128_error_fn_mmd_rbf_all_median_quantile_0.95\n",
      "TPR: 0.69 | TNR: 0.55 | Acc: 0.62 | Bal. Acc: 0.62\n",
      "TP Time 127.33 (83.51)\n",
      "\n",
      "Episode Results: ep_iid_cum | pred_horizon_16_sample_size_128_error_fn_kde_kl_all_for_eig_quantile_0.95\n",
      "TPR: 0.70 | TNR: 0.55 | Acc: 0.63 | Bal. Acc: 0.62\n",
      "TP Time 136.07 (84.02)\n"
     ]
    }
   ],
   "source": [
    "# Run eval\n",
    "output_data = evaluate_temporal_consistency(cfg, dataset_path, dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b03d4",
   "metadata": {},
   "source": [
    "# Wasserstein metric demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46dfcc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ot\n",
    "import numpy as np\n",
    "\n",
    "def compute_wasserstein_ot(x: np.ndarray, y: np.ndarray, p: int = 1) -> float:\n",
    "    \"\"\"\n",
    "    Exact p-Wasserstein distance between two empirical measures on R^D.\n",
    "\n",
    "    Args:\n",
    "      x: [N, D] array \n",
    "      y: [M, D] array\n",
    "      p: ground‐metric exponent (1 for 1-Wasserstein, 2 for 2-Wasserstein)\n",
    "\n",
    "    Returns:\n",
    "      W_p(x, y)\n",
    "    \"\"\"\n",
    "    n, m = x.shape[0], y.shape[0]\n",
    "    a = np.ones(n) / n   # uniform weights over x\n",
    "    b = np.ones(m) / m   # uniform weights over y\n",
    "\n",
    "    # cost matrix: ||x_i - y_j||_2^p\n",
    "    M = ot.dist(x, y, metric='euclidean')**p  # shape [128,128]\n",
    "\n",
    "    # emd2 returns the p-th power of W_p\n",
    "    Wp_p = ot.emd2(a, b, M)\n",
    "    return float(Wp_p**(1.0/p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de61eb9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203.59145381588658"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(128,16)\n",
    "y = np.random.randn(128,16) * 10 + 50\n",
    "\n",
    "compute_wasserstein_ot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95dd380",
   "metadata": {},
   "source": [
    "## Embedding Detector Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "247fcd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SCORE_FNS: Dict[str, Dict[str, Any]] = {\n",
    "    \"top1_l2\": {\"method\": \"topk\", \"method_kwargs\": {\"error_fn\": \"l2\", \"k\": 1}},\n",
    "    \"top5_l2\": {\"method\": \"topk\", \"method_kwargs\": {\"error_fn\": \"l2\", \"k\": 5}},\n",
    "    \"top10_l2\": {\"method\": \"topk\", \"method_kwargs\": {\"error_fn\": \"l2\", \"k\": 10}},\n",
    "    \"top1_cosine\": {\"method\": \"topk\", \"method_kwargs\": {\"error_fn\": \"cosine\", \"k\": 1}},\n",
    "    \"top5_cosine\": {\"method\": \"topk\", \"method_kwargs\": {\"error_fn\": \"cosine\", \"k\": 5}},\n",
    "    \"top10_cosine\": {\n",
    "        \"method\": \"topk\",\n",
    "        \"method_kwargs\": {\"error_fn\": \"cosine\", \"k\": 10},\n",
    "    },\n",
    "    \"mahal\": {\n",
    "        \"method\": \"mahal\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def embedding_similarity_exp_key(\n",
    "    embedding: str,\n",
    "    score_fn: int,\n",
    ") -> str:\n",
    "    return f\"embedding_{embedding}_score_fn_{score_fn}\"\n",
    "\n",
    "def get_rgb(data: Dict[str, Any]) -> Optional[np.ndarray]:\n",
    "    return data['rgb'][0]\n",
    "\n",
    "# Custom function that returns the average latent embedding for one time step\n",
    "# visual conditioning has two in the obs-history w current setup\n",
    "def get_latent_embedding(data, average=True):\n",
    "    image_feats = data['step_image_features']\n",
    "    agent_pos = data['step_agent_poses']\n",
    "\n",
    "    comb = np.concatenate([image_feats, agent_pos], axis=-1)\n",
    "\n",
    "    if average:\n",
    "        return comb.mean(axis=0)\n",
    "\n",
    "    return comb[-1]\n",
    "\n",
    "def compute_embedding_similarity_scores(\n",
    "    cfg: omegaconf.DictConfig,\n",
    "    test_dataset: Optional[EpisodeDataset] = None,\n",
    "    demo_dataset: Optional[EpisodeDataset] = None,\n",
    "    demo_frame: Optional[pd.DataFrame] = None,\n",
    "    leave_timestep_out: bool = False,\n",
    "    leave_episode_out: bool = False,\n",
    "    demo_as_test: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute embedding similarity scores over dataset.\"\"\"\n",
    "    assert not (leave_timestep_out and leave_episode_out)\n",
    "    assert (demo_dataset is not None) ^ (demo_frame is not None)\n",
    "    assert (test_dataset is not None) ^ demo_as_test\n",
    "\n",
    "    # Extract demo embeddings.\n",
    "    if demo_dataset is not None:\n",
    "        demo_frame = []\n",
    "        for data in iter(demo_dataset):\n",
    "            demo_frame.append(\n",
    "                {\n",
    "                    \"episode\": data[\"episode\"],\n",
    "                    \"timestep\": data[\"timestep\"],\n",
    "                    \"success\": data.get(\"success\", True),\n",
    "                }\n",
    "            )\n",
    "            rgb = get_rgb(data)\n",
    "            if isinstance(rgb, np.ndarray):\n",
    "                demo_frame[-1][\"rgb\"] = rgb\n",
    "\n",
    "            for embedding in cfg.eval.embedding.embeddings:\n",
    "                if embedding == \"step_obs\":\n",
    "                    demo_frame[-1][embedding] = get_latent_embedding(data)\n",
    "                else:\n",
    "                    demo_frame[-1][embedding] = data[embedding].flatten()\n",
    "\n",
    "        demo_frame = pd.DataFrame(demo_frame)\n",
    "    assert isinstance(demo_frame, pd.DataFrame)\n",
    "\n",
    "    # Extract test embeddings.\n",
    "    if demo_as_test:\n",
    "        test_frame = demo_frame.copy()\n",
    "    else:\n",
    "        test_frame = []\n",
    "        for data in iter(test_dataset):\n",
    "            test_frame.append(\n",
    "                {\n",
    "                    \"episode\": data[\"episode\"],\n",
    "                    \"timestep\": data[\"timestep\"],\n",
    "                    \"success\": data.get(\"success\", True),\n",
    "                }\n",
    "            )\n",
    "            rgb = get_rgb(data)\n",
    "            if isinstance(rgb, np.ndarray):\n",
    "                test_frame[-1][\"rgb\"] = rgb\n",
    "\n",
    "            for embedding in cfg.eval.embedding.embeddings:\n",
    "                if embedding == \"step_obs\":\n",
    "                    test_frame[-1][embedding] = get_latent_embedding(data)\n",
    "                else:\n",
    "                    test_frame[-1][embedding] = data[embedding].flatten()\n",
    "\n",
    "        test_frame = pd.DataFrame(test_frame)\n",
    "    assert isinstance(test_frame, pd.DataFrame)\n",
    "\n",
    "    # Compute embedding scores.\n",
    "    exp_keys = []\n",
    "    for embedding in cfg.eval.embedding.embeddings:\n",
    "        for score_fn in cfg.eval.embedding.score_fns:\n",
    "\n",
    "            exp_key = embedding_similarity_exp_key(\n",
    "                embedding=embedding,\n",
    "                score_fn=score_fn,\n",
    "            )\n",
    "            if exp_key not in exp_keys:\n",
    "                exp_keys.append(exp_key)\n",
    "\n",
    "            if leave_episode_out:\n",
    "                test_frame = pd.concat(\n",
    "                    [\n",
    "                        test_frame,\n",
    "                        pd.Series(np.zeros(len(test_frame)), name=f\"{exp_key}_score\"),\n",
    "                    ],\n",
    "                    axis=1,\n",
    "                )\n",
    "                for i in range(data_utils.num_episodes(test_frame)):\n",
    "                    episode_frame = data_utils.get_episode(\n",
    "                        test_frame, i, use_index=True\n",
    "                    )\n",
    "                    episode = episode_frame.iloc[0].to_dict()[\"episode\"]\n",
    "                    non_episode_frame: pd.DataFrame = demo_frame[\n",
    "                        demo_frame[\"episode\"] != episode\n",
    "                    ]\n",
    "\n",
    "                    episode_scores = error_utils.compute_embedding_scores(\n",
    "                        data_embeddings=non_episode_frame[embedding].values,\n",
    "                        test_embeddings=episode_frame[embedding].values,\n",
    "                        **EMBEDDING_SCORE_FNS[score_fn],\n",
    "                    )\n",
    "                    test_frame.loc[\n",
    "                        test_frame[\"episode\"] == episode, f\"{exp_key}_score\"\n",
    "                    ] = episode_scores\n",
    "            else:\n",
    "                test_scores = error_utils.compute_embedding_scores(\n",
    "                    data_embeddings=demo_frame[embedding].values,\n",
    "                    test_embeddings=test_frame[embedding].values,\n",
    "                    leave_one_out=leave_timestep_out,\n",
    "                    **EMBEDDING_SCORE_FNS[score_fn],\n",
    "                )\n",
    "                test_frame = pd.concat(\n",
    "                    [test_frame, pd.Series(test_scores, name=f\"{exp_key}_score\")],\n",
    "                    axis=1,\n",
    "                )\n",
    "\n",
    "    test_frame = compute_cum_scores(test_frame, exp_keys)\n",
    "    return test_frame\n",
    "\n",
    "\n",
    "def evaluate_embedding_similarity(\n",
    "    cfg: omegaconf.DictConfig,\n",
    "    demo_dataset_path: Union[str, pathlib.Path],\n",
    "    test_dataset_path: Union[str, pathlib.Path],\n",
    ") -> Dict[str, Union[Dict[str, Any], pd.DataFrame]]:\n",
    "    \"\"\"Compute embedding similarity results.\"\"\"\n",
    "    # Construct episode iterable datasets.\n",
    "    demo_dataset = EpisodeDataset(\n",
    "        dataset_path=demo_dataset_path,\n",
    "        exec_horizon=1,\n",
    "        sample_history=0,\n",
    "        filter_success=getattr(cfg.eval, \"filter_demo_success\", False),\n",
    "        filter_failure=getattr(cfg.eval, \"filter_demo_failure\", True),\n",
    "        filter_episodes=getattr(cfg.eval, \"filter_demo_episodes\", None),\n",
    "        max_episode_length=getattr(cfg.eval, \"max_demo_episode_length\", None),\n",
    "        max_num_episodes=getattr(cfg.eval, \"max_num_episodes\", None),\n",
    "    )\n",
    "    test_dataset = EpisodeDataset(\n",
    "        dataset_path=test_dataset_path,\n",
    "        exec_horizon=1,\n",
    "        sample_history=0,\n",
    "        filter_success=getattr(cfg.eval, \"filter_test_success\", False),\n",
    "        filter_failure=getattr(cfg.eval, \"filter_test_failure\", False),\n",
    "        filter_episodes=getattr(cfg.eval, \"filter_test_episodes\", None),\n",
    "        max_episode_length=getattr(cfg.eval, \"max_test_episode_length\", None),\n",
    "        max_num_episodes=getattr(cfg.eval, \"max_num_episodes\", None),\n",
    "    )\n",
    "\n",
    "    # Compute scores for specified parameter sets.\n",
    "    results_dict = defaultdict(dict)\n",
    "    demo_results_frame = compute_embedding_similarity_scores(\n",
    "        cfg,\n",
    "        demo_dataset=demo_dataset,\n",
    "        demo_as_test=True,\n",
    "        leave_episode_out=getattr(cfg.eval.embedding, \"leave_episode_out\", True),\n",
    "        leave_timestep_out=getattr(cfg.eval.embedding, \"leave_timestep_out\", False),\n",
    "    )\n",
    "    test_results_frame = compute_embedding_similarity_scores(\n",
    "        cfg,\n",
    "        test_dataset=test_dataset,\n",
    "        demo_frame=demo_results_frame,\n",
    "    )\n",
    "\n",
    "    # Compute metrics for specified parameter sets.\n",
    "    for embedding in cfg.eval.embedding.embeddings:\n",
    "        for score_fn in cfg.eval.embedding.score_fns:\n",
    "\n",
    "            exp_key = embedding_similarity_exp_key(\n",
    "                embedding=embedding,\n",
    "                score_fn=score_fn,\n",
    "            )\n",
    "\n",
    "            for quantile in cfg.eval.quantiles:\n",
    "\n",
    "                test_results_frame = metric_utils.compute_detection_results(\n",
    "                    exp_key=exp_key,\n",
    "                    quantile_key=quantile_exp_key(exp_key, quantile),\n",
    "                    results_dict=results_dict,\n",
    "                    demo_results_frame=demo_results_frame,\n",
    "                    test_results_frame=test_results_frame,\n",
    "                    detector=getattr(cfg.eval, \"detector\", \"quantile\"),\n",
    "                    detector_kwargs={\n",
    "                        \"quantile\": quantile,\n",
    "                        **getattr(cfg.eval, \"detector_kwargs\", {}),\n",
    "                    },\n",
    "                )\n",
    "\n",
    "    return {\n",
    "        \"results_dict\": results_dict,\n",
    "        \"test_results_frame\": test_results_frame,\n",
    "        \"demo_results_frame\": demo_results_frame,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0203d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dict = {\n",
    "    \"env\": {\n",
    "        \"args\": {\n",
    "            \"freq\": 1,\n",
    "            \"max_episode_length\": 300\n",
    "        },\n",
    "        \"dof\": 2,\n",
    "        \"num_eef\": 1\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"ac_horizon\": 8,\n",
    "    },\n",
    "    \"eval\": {\n",
    "        \"consistency\": {\n",
    "            \"sample_sizes\": [128, 256],\n",
    "            \"error_fns\": [\"mse_all\", \"mmd_rbf_all_median\", \"kde_kl_all_for_eig\"], \n",
    "            \"pred_horizons\": [16],\n",
    "            \"aggr_fns\": [\"min\"]\n",
    "        },\n",
    "        \"quantiles\": [0.95],\n",
    "        \"embedding\": {\n",
    "            \"embeddings\": [\"step_obs\"],\n",
    "            \"score_fns\": [\"top5_cosine\", \"top10_cosine\", \"mahal\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg = omegaconf.OmegaConf.create(conf_dict)\n",
    "\n",
    "demo_dataset_path = \"logs/datasets/no_domain_randomization_v8_simple_env\"\n",
    "test_dataset_path = \"logs/datasets/domain_randomization_v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "822e7d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Results: ep_iid_cum | embedding_step_obs_score_fn_top5_cosine_quantile_0.95\n",
      "TPR: 0.03 | TNR: 0.90 | Acc: 0.20 | Bal. Acc: 0.46\n",
      "TP Time 0.00 (0.00)\n",
      "\n",
      "Episode Results: ep_iid_cum | embedding_step_obs_score_fn_top10_cosine_quantile_0.95\n",
      "TPR: 0.03 | TNR: 1.00 | Acc: 0.22 | Bal. Acc: 0.51\n",
      "TP Time 0.00 (0.00)\n",
      "\n",
      "Episode Results: ep_iid_cum | embedding_step_obs_score_fn_mahal_quantile_0.95\n",
      "TPR: 1.00 | TNR: 1.00 | Acc: 1.00 | Bal. Acc: 1.00\n",
      "TP Time 220.60 (17.05)\n"
     ]
    }
   ],
   "source": [
    "# Run eval\n",
    "output_data = evaluate_embedding_similarity(cfg, demo_dataset_path, test_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6920fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"logs/datasets/no_domain_randomization_v8_simple_env/episode_000_failure.pkl\")\n",
    "\n",
    "img_feats = df.step_image_features[0]\n",
    "agent_pos = df.step_agent_poses[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b28689e",
   "metadata": {},
   "source": [
    "## Testing the diffusion variance code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "187825e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSEMBLE_ACTION_SPACES: Dict[str, Dict[str, Any]] = {\n",
    "    \"all\": {\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "    },\n",
    "    \"pos\": {\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": True,\n",
    "    },\n",
    "    \"traj\": {\"ignore_gripper\": True, \"ignore_rotation\": True, \"use_trajectory\": True},\n",
    "}\n",
    "\n",
    "def diffusion_ensemble_exp_key(\n",
    "    pred_horizon: int,\n",
    "    sample_size: int,\n",
    "    action_space: str,\n",
    ") -> str:\n",
    "    return f\"pred_horizon_{pred_horizon}_sample_size_{sample_size}_action_space_{action_space}\"\n",
    "\n",
    "\n",
    "def compute_diffusion_ensemble_variances(\n",
    "    cfg: omegaconf.DictConfig,\n",
    "    dataset: EpisodeDataset,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute diffusion ensemble variances over dataset.\"\"\"\n",
    "    results = []\n",
    "    exp_keys = []\n",
    "\n",
    "    for data in iter(dataset):\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"episode\": data[\"episode\"],\n",
    "                \"timestep\": data[\"timestep\"],\n",
    "                \"success\": data.get(\"success\", True),\n",
    "            }\n",
    "        )\n",
    "        rgb = get_rgb(data)\n",
    "        if isinstance(rgb, np.ndarray):\n",
    "            results[-1][\"rgb\"] = rgb\n",
    "\n",
    "        for sample_size in cfg.eval.ensemble.sample_sizes:\n",
    "\n",
    "            # Subsample current actions.\n",
    "            actions = action_utils.subsample_actions(\n",
    "                data[\"sampled_actions\"],\n",
    "                sample_size,\n",
    "            )\n",
    "\n",
    "            for pred_horizon in cfg.eval.ensemble.pred_horizons:\n",
    "                for action_space in cfg.eval.ensemble.action_spaces:\n",
    "\n",
    "                    exp_key = diffusion_ensemble_exp_key(\n",
    "                        pred_horizon=pred_horizon,\n",
    "                        sample_size=sample_size,\n",
    "                        action_space=action_space,\n",
    "                    )\n",
    "                    if exp_key not in exp_keys:\n",
    "                        exp_keys.append(exp_key)\n",
    "\n",
    "                    # Compute variance (vectorized).\n",
    "                    variance = error_utils.compute_action_variance(\n",
    "                        actions=actions,\n",
    "                        pred_horizon=pred_horizon,\n",
    "                        sim_freq=cfg.env.args.freq,\n",
    "                        num_robots=cfg.env.num_eef,\n",
    "                        action_dim=cfg.env.dof,\n",
    "                        **ENSEMBLE_ACTION_SPACES[action_space],\n",
    "                    )\n",
    "                    results[-1][f\"{exp_key}_score\"] = variance\n",
    "\n",
    "    results_frame = compute_cum_scores(pd.DataFrame(results), exp_keys)\n",
    "    return results_frame\n",
    "\n",
    "\n",
    "def evaluate_diffusion_ensemble(\n",
    "    cfg: omegaconf.DictConfig,\n",
    "    demo_dataset_path: Union[str, pathlib.Path],\n",
    "    test_dataset_path: Union[str, pathlib.Path],\n",
    ") -> Dict[str, Union[Dict[str, Any], pd.DataFrame]]:\n",
    "    \"\"\"Compute diffusion ensemble results.\"\"\"\n",
    "    # Construct episode iterable datasets.\n",
    "    demo_dataset = EpisodeDataset(\n",
    "        dataset_path=demo_dataset_path,\n",
    "        exec_horizon=1,\n",
    "        sample_history=0,\n",
    "        filter_success=getattr(cfg.eval, \"filter_demo_success\", False),\n",
    "        filter_failure=getattr(cfg.eval, \"filter_demo_failure\", True),\n",
    "        filter_episodes=getattr(cfg.eval, \"filter_demo_episodes\", None),\n",
    "        max_episode_length=getattr(cfg.eval, \"max_demo_episode_length\", None),\n",
    "        max_num_episodes=getattr(cfg.eval, \"max_num_episodes\", None),\n",
    "    )\n",
    "    test_dataset = EpisodeDataset(\n",
    "        dataset_path=test_dataset_path,\n",
    "        exec_horizon=1,\n",
    "        sample_history=0,\n",
    "        filter_success=getattr(cfg.eval, \"filter_test_success\", False),\n",
    "        filter_failure=getattr(cfg.eval, \"filter_test_failure\", False),\n",
    "        filter_episodes=getattr(cfg.eval, \"filter_test_episodes\", None),\n",
    "        max_episode_length=getattr(cfg.eval, \"max_test_episode_length\", None),\n",
    "        max_num_episodes=getattr(cfg.eval, \"max_num_episodes\", None),\n",
    "    )\n",
    "\n",
    "    # Compute scores for specified parameter sets.\n",
    "    results_dict = defaultdict(dict)\n",
    "    demo_results_frame = compute_diffusion_ensemble_variances(cfg, demo_dataset)\n",
    "    test_results_frame = compute_diffusion_ensemble_variances(cfg, test_dataset)\n",
    "\n",
    "    # Compute metrics for specified parameter sets.\n",
    "    for sample_size in cfg.eval.ensemble.sample_sizes:\n",
    "        for pred_horizon in cfg.eval.ensemble.pred_horizons:\n",
    "            for action_space in cfg.eval.ensemble.action_spaces:\n",
    "\n",
    "                exp_key = diffusion_ensemble_exp_key(\n",
    "                    pred_horizon=pred_horizon,\n",
    "                    sample_size=sample_size,\n",
    "                    action_space=action_space,\n",
    "                )\n",
    "\n",
    "                for quantile in cfg.eval.quantiles:\n",
    "\n",
    "                    test_results_frame = metric_utils.compute_detection_results(\n",
    "                        exp_key=exp_key,\n",
    "                        quantile_key=quantile_exp_key(exp_key, quantile),\n",
    "                        results_dict=results_dict,\n",
    "                        demo_results_frame=demo_results_frame,\n",
    "                        test_results_frame=test_results_frame,\n",
    "                        detector=getattr(cfg.eval, \"detector\", \"quantile\"),\n",
    "                        detector_kwargs={\n",
    "                            \"quantile\": quantile,\n",
    "                            **getattr(cfg.eval, \"detector_kwargs\", {}),\n",
    "                        },\n",
    "                    )\n",
    "\n",
    "    return {\n",
    "        \"results_dict\": results_dict,\n",
    "        \"test_results_frame\": test_results_frame,\n",
    "        \"demo_results_frame\": demo_results_frame,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d23ee145",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dict = {\n",
    "    \"env\": {\n",
    "        \"args\": {\n",
    "            \"freq\": 1,\n",
    "            \"max_episode_length\": 300\n",
    "        },\n",
    "        \"dof\": 2,\n",
    "        \"num_eef\": 1\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"ac_horizon\": 8,\n",
    "    },\n",
    "    \"eval\": {\n",
    "        \"consistency\": {\n",
    "            \"sample_sizes\": [128, 256],\n",
    "            \"error_fns\": [\"mse_all\", \"mmd_rbf_all_median\", \"kde_kl_all_for_eig\"], \n",
    "            \"pred_horizons\": [16],\n",
    "            \"aggr_fns\": [\"min\"]\n",
    "        },\n",
    "        \"quantiles\": [0.95],\n",
    "        \"embedding\": {\n",
    "            \"embeddings\": [\"step_obs\"],\n",
    "            \"score_fns\": [\"top5_cosine\", \"top10_cosine\", \"mahal\", \"\"]\n",
    "        },\n",
    "        \"ensemble\": {\n",
    "            \"sample_sizes\": [128, 256],\n",
    "            \"pred_horizons\": [16],\n",
    "            \"action_spaces\": [\"traj\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "cfg = omegaconf.OmegaConf.create(conf_dict)\n",
    "\n",
    "demo_dataset_path = \"logs/datasets/no_domain_randomization_v8_simple_env\"\n",
    "test_dataset_path = \"logs/datasets/domain_randomization_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da318524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval code\n",
    "output_data = evaluate_diffusion_ensemble(cfg, demo_dataset_path, test_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec72d9a",
   "metadata": {},
   "source": [
    "## Testing Entropy distance measure\n",
    "- Ablation on action diffusion ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5460a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_ERROR_FNS = {\n",
    "    \"knn_5\": {\n",
    "        \"error_fn\": \"knn\",\n",
    "        \"k\": 5,\n",
    "    },\n",
    "    \"knn_10\": {\n",
    "        \"error_fn\": \"knn\",\n",
    "        \"k\": 10,\n",
    "    },\n",
    "    \"gmm_5\": {\n",
    "        \"error_fn\": \"gmm\",\n",
    "        \"n_components\": 5\n",
    "    },\n",
    "    \"kde_max_eig\": {\n",
    "        \"error_fn\": \"kde\",\n",
    "        \"bandwidth\": \"max_eig\"\n",
    "    },\n",
    "}\n",
    "\n",
    "def entropy_ensemble_exp_key(\n",
    "    error_fn: int,\n",
    "    sample_size: int,\n",
    "    pred_horizon: int,\n",
    ") -> str:\n",
    "    return f\"error_fn_{error_fn}_sample_size_{sample_size}_pred_horizon_{pred_horizon}\"\n",
    "\n",
    "\n",
    "def compute_diffusion_ensemble_entropy(\n",
    "    cfg: omegaconf.DictConfig,\n",
    "    dataset: EpisodeDataset,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute diffusion ensemble variances over dataset.\"\"\"\n",
    "    results = []\n",
    "    exp_keys = []\n",
    "\n",
    "    for data in iter(dataset):\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"episode\": data[\"episode\"],\n",
    "                \"timestep\": data[\"timestep\"],\n",
    "                \"success\": data.get(\"success\", True),\n",
    "            }\n",
    "        )\n",
    "        rgb = get_rgb(data)\n",
    "        if isinstance(rgb, np.ndarray):\n",
    "            results[-1][\"rgb\"] = rgb\n",
    "\n",
    "        for sample_size in cfg.eval.entropy.sample_sizes:\n",
    "\n",
    "            # Subsample current actions.\n",
    "            actions = action_utils.subsample_actions(\n",
    "                data[\"sampled_actions\"],\n",
    "                sample_size,\n",
    "            )\n",
    "\n",
    "            for pred_horizon in cfg.eval.entropy.pred_horizons:\n",
    "                for error_fn in cfg.eval.entropy.error_fns:\n",
    "\n",
    "                    exp_key = entropy_ensemble_exp_key(\n",
    "                        error_fn=error_fn,\n",
    "                        sample_size=sample_size,\n",
    "                        pred_horizon=pred_horizon,\n",
    "                    )\n",
    "                    if exp_key not in exp_keys:\n",
    "                        exp_keys.append(exp_key)\n",
    "\n",
    "                    # Compute variance (vectorized).\n",
    "                    entropy = error_utils.compute_action_entropy(\n",
    "                        actions=actions,\n",
    "                        pred_horizon=pred_horizon,\n",
    "                        **ENTROPY_ERROR_FNS[error_fn],\n",
    "                    )\n",
    "                    results[-1][f\"{exp_key}_score\"] = entropy\n",
    "\n",
    "    results_frame = compute_cum_scores(pd.DataFrame(results), exp_keys)\n",
    "    return results_frame\n",
    "\n",
    "def evaluate_diffusion_entropy(\n",
    "    cfg: omegaconf.DictConfig,\n",
    "    demo_dataset_path: Union[str, pathlib.Path],\n",
    "    test_dataset_path: Union[str, pathlib.Path],\n",
    ") -> Dict[str, Union[Dict[str, Any], pd.DataFrame]]:\n",
    "    \"\"\"Compute diffusion ensemble results.\"\"\"\n",
    "    # Construct episode iterable datasets.\n",
    "    demo_dataset = EpisodeDataset(\n",
    "        dataset_path=demo_dataset_path,\n",
    "        exec_horizon=1,\n",
    "        sample_history=0,\n",
    "        filter_success=getattr(cfg.eval, \"filter_demo_success\", False),\n",
    "        filter_failure=getattr(cfg.eval, \"filter_demo_failure\", True),\n",
    "        filter_episodes=getattr(cfg.eval, \"filter_demo_episodes\", None),\n",
    "        max_episode_length=getattr(cfg.eval, \"max_demo_episode_length\", None),\n",
    "        max_num_episodes=getattr(cfg.eval, \"max_num_episodes\", None),\n",
    "    )\n",
    "    test_dataset = EpisodeDataset(\n",
    "        dataset_path=test_dataset_path,\n",
    "        exec_horizon=1,\n",
    "        sample_history=0,\n",
    "        filter_success=getattr(cfg.eval, \"filter_test_success\", False),\n",
    "        filter_failure=getattr(cfg.eval, \"filter_test_failure\", False),\n",
    "        filter_episodes=getattr(cfg.eval, \"filter_test_episodes\", None),\n",
    "        max_episode_length=getattr(cfg.eval, \"max_test_episode_length\", None),\n",
    "        max_num_episodes=getattr(cfg.eval, \"max_num_episodes\", None),\n",
    "    )\n",
    "\n",
    "    # Compute scores for specified parameter sets.\n",
    "    results_dict = defaultdict(dict)\n",
    "    demo_results_frame = compute_diffusion_ensemble_entropy(cfg, demo_dataset)\n",
    "    test_results_frame = compute_diffusion_ensemble_entropy(cfg, test_dataset)\n",
    "\n",
    "    # Compute metrics for specified parameter sets.\n",
    "    for sample_size in cfg.eval.entropy.sample_sizes:\n",
    "        for pred_horizon in cfg.eval.entropy.pred_horizons:\n",
    "            for error_fn in cfg.eval.entropy.error_fns:\n",
    "\n",
    "                exp_key = entropy_ensemble_exp_key(\n",
    "                        error_fn=error_fn,\n",
    "                        sample_size=sample_size,\n",
    "                        pred_horizon=pred_horizon,\n",
    "                    )\n",
    "\n",
    "                for quantile in cfg.eval.quantiles:\n",
    "\n",
    "                    test_results_frame = metric_utils.compute_detection_results(\n",
    "                        exp_key=exp_key,\n",
    "                        quantile_key=quantile_exp_key(exp_key, quantile),\n",
    "                        results_dict=results_dict,\n",
    "                        demo_results_frame=demo_results_frame,\n",
    "                        test_results_frame=test_results_frame,\n",
    "                        detector=getattr(cfg.eval, \"detector\", \"quantile\"),\n",
    "                        detector_kwargs={\n",
    "                            \"quantile\": quantile,\n",
    "                            **getattr(cfg.eval, \"detector_kwargs\", {}),\n",
    "                        },\n",
    "                    )\n",
    "\n",
    "    return {\n",
    "        \"results_dict\": results_dict,\n",
    "        \"test_results_frame\": test_results_frame,\n",
    "        \"demo_results_frame\": demo_results_frame,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dict = {\n",
    "    \"env\": {\n",
    "        \"args\": {\n",
    "            \"freq\": 1,\n",
    "            \"max_episode_length\": 300\n",
    "        },\n",
    "        \"dof\": 2,\n",
    "        \"num_eef\": 1\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"ac_horizon\": 8,\n",
    "    },\n",
    "    \"eval\": {\n",
    "        \"consistency\": {\n",
    "            \"sample_sizes\": [128, 256],\n",
    "            \"error_fns\": [\"mse_all\", \"mmd_rbf_all_median\", \"kde_kl_all_for_eig\"], \n",
    "            \"pred_horizons\": [16],\n",
    "            \"aggr_fns\": [\"min\"]\n",
    "        },\n",
    "        \"quantiles\": [0.95],\n",
    "        \"embedding\": {\n",
    "            \"embeddings\": [\"step_obs\"],\n",
    "            \"score_fns\": [\"top5_cosine\", \"top10_cosine\", \"mahal\", \"\"]\n",
    "        },\n",
    "        \"ensemble\": {\n",
    "            \"sample_sizes\": [128, 256],\n",
    "            \"pred_horizons\": [16],\n",
    "            \"action_spaces\": [\"traj\"]\n",
    "        },\n",
    "        \"entropy\": {\n",
    "            \"sample_sizes\": [128, 256],\n",
    "            \"pred_horizons\": [16],\n",
    "            \"error_fns\": [\"gmm_5\", \"kde_max_eig\", \"knn_5\"]\n",
    "        },\n",
    "        \"dummy_score\": {\n",
    "            \"constant\": 0.1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg = omegaconf.OmegaConf.create(conf_dict)\n",
    "\n",
    "demo_dataset_path = \"logs/datasets/no_domain_randomization_v8_simple_env\"\n",
    "test_dataset_path = \"logs/datasets/domain_randomization_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c188957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Results: ep_iid_cum | error_fn_gmm_5_sample_size_128_pred_horizon_16_quantile_0.95\n",
      "TPR: 0.12 | TNR: 1.00 | Acc: 0.30 | Bal. Acc: 0.56\n",
      "TP Time 0.00 (0.00)\n",
      "\n",
      "Episode Results: ep_iid_cum | error_fn_kde_max_eig_sample_size_128_pred_horizon_16_quantile_0.95\n",
      "TPR: 0.50 | TNR: 1.00 | Acc: 0.60 | Bal. Acc: 0.75\n",
      "TP Time 286.80 (9.89)\n",
      "\n",
      "Episode Results: ep_iid_cum | error_fn_knn_5_sample_size_128_pred_horizon_16_quantile_0.95\n",
      "TPR: 0.78 | TNR: 1.00 | Acc: 0.82 | Bal. Acc: 0.89\n",
      "TP Time 286.19 (11.80)\n",
      "\n",
      "Episode Results: ep_iid_cum | error_fn_gmm_5_sample_size_256_pred_horizon_16_quantile_0.95\n",
      "TPR: 0.07 | TNR: 0.90 | Acc: 0.24 | Bal. Acc: 0.49\n",
      "TP Time 170.67 (82.19)\n",
      "\n",
      "Episode Results: ep_iid_cum | error_fn_kde_max_eig_sample_size_256_pred_horizon_16_quantile_0.95\n",
      "TPR: 0.50 | TNR: 1.00 | Acc: 0.60 | Bal. Acc: 0.75\n",
      "TP Time 286.80 (9.89)\n",
      "\n",
      "Episode Results: ep_iid_cum | error_fn_knn_5_sample_size_256_pred_horizon_16_quantile_0.95\n",
      "TPR: 0.78 | TNR: 1.00 | Acc: 0.82 | Bal. Acc: 0.89\n",
      "TP Time 286.19 (11.80)\n"
     ]
    }
   ],
   "source": [
    "# Eval code\n",
    "output_data = evaluate_diffusion_entropy(cfg, demo_dataset_path, test_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0edb8",
   "metadata": {},
   "source": [
    "## Constant Dummy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6165b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_exp_key(constant) -> str:\n",
    "    return f\"dummy_score_{constant}\"\n",
    "\n",
    "def compute_dummy_score(\n",
    "    cfg: omegaconf.DictConfig,\n",
    "    dataset: EpisodeDataset,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute diffusion ensemble variances over dataset.\"\"\"\n",
    "    results = []\n",
    "    exp_keys = []\n",
    "\n",
    "    for data in iter(dataset):\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"episode\": data[\"episode\"],\n",
    "                \"timestep\": data[\"timestep\"],\n",
    "                \"success\": data.get(\"success\", True),\n",
    "            }\n",
    "        )\n",
    "        rgb = get_rgb(data)\n",
    "        if isinstance(rgb, np.ndarray):\n",
    "            results[-1][\"rgb\"] = rgb\n",
    "        \n",
    "        constant=cfg.eval.dummy_score.constant\n",
    "\n",
    "        exp_key = dummy_exp_key(\n",
    "            constant=constant\n",
    "        )\n",
    "\n",
    "        if exp_key not in exp_keys:\n",
    "            exp_keys.append(exp_key)\n",
    "\n",
    "        # Compute variance (vectorized).\n",
    "        results[-1][f\"{exp_key}_score\"] = constant\n",
    "\n",
    "    results_frame = compute_cum_scores(pd.DataFrame(results), exp_keys)\n",
    "    return results_frame\n",
    "\n",
    "def evaluate_dummy_score(\n",
    "    cfg: omegaconf.DictConfig,\n",
    "    demo_dataset_path: Union[str, pathlib.Path],\n",
    "    test_dataset_path: Union[str, pathlib.Path],\n",
    ") -> Dict[str, Union[Dict[str, Any], pd.DataFrame]]:\n",
    "    \"\"\"Compute diffusion ensemble results.\"\"\"\n",
    "    # Construct episode iterable datasets.\n",
    "    demo_dataset = EpisodeDataset(\n",
    "        dataset_path=demo_dataset_path,\n",
    "        exec_horizon=1,\n",
    "        sample_history=0,\n",
    "        filter_success=getattr(cfg.eval, \"filter_demo_success\", False),\n",
    "        filter_failure=getattr(cfg.eval, \"filter_demo_failure\", True),\n",
    "        filter_episodes=getattr(cfg.eval, \"filter_demo_episodes\", None),\n",
    "        max_episode_length=getattr(cfg.eval, \"max_demo_episode_length\", None),\n",
    "        max_num_episodes=getattr(cfg.eval, \"max_num_episodes\", None),\n",
    "    )\n",
    "    test_dataset = EpisodeDataset(\n",
    "        dataset_path=test_dataset_path,\n",
    "        exec_horizon=1,\n",
    "        sample_history=0,\n",
    "        filter_success=getattr(cfg.eval, \"filter_test_success\", False),\n",
    "        filter_failure=getattr(cfg.eval, \"filter_test_failure\", False),\n",
    "        filter_episodes=getattr(cfg.eval, \"filter_test_episodes\", None),\n",
    "        max_episode_length=getattr(cfg.eval, \"max_test_episode_length\", None),\n",
    "        max_num_episodes=getattr(cfg.eval, \"max_num_episodes\", None),\n",
    "    )\n",
    "\n",
    "    # Compute scores for specified parameter sets.\n",
    "    results_dict = defaultdict(dict)\n",
    "    demo_results_frame = compute_dummy_score(cfg, demo_dataset)\n",
    "    test_results_frame = compute_dummy_score(cfg, test_dataset)\n",
    "\n",
    "    # Compute metrics for specified parameter sets.\n",
    "\n",
    "    exp_key = dummy_exp_key(\n",
    "            constant=cfg.eval.dummy_score.constant\n",
    "    )\n",
    "\n",
    "    for quantile in cfg.eval.quantiles:\n",
    "        test_results_frame = metric_utils.compute_detection_results(\n",
    "            exp_key=exp_key,\n",
    "            quantile_key=quantile_exp_key(exp_key, quantile),\n",
    "            results_dict=results_dict,\n",
    "            demo_results_frame=demo_results_frame,\n",
    "            test_results_frame=test_results_frame,\n",
    "            detector=getattr(cfg.eval, \"detector\", \"quantile\"),\n",
    "            detector_kwargs={\n",
    "                \"quantile\": quantile,\n",
    "                **getattr(cfg.eval, \"detector_kwargs\", {}),\n",
    "            },\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"results_dict\": results_dict,\n",
    "        \"test_results_frame\": test_results_frame,\n",
    "        \"demo_results_frame\": demo_results_frame,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dict = {\n",
    "    \"env\": {\n",
    "        \"args\": {\n",
    "            \"freq\": 1,\n",
    "            \"max_episode_length\": 300\n",
    "        },\n",
    "        \"dof\": 2,\n",
    "        \"num_eef\": 1\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"ac_horizon\": 8,\n",
    "    },\n",
    "    \"eval\": {\n",
    "        \"consistency\": {\n",
    "            \"sample_sizes\": [128, 256],\n",
    "            \"error_fns\": [\"mse_all\", \"mmd_rbf_all_median\", \"kde_kl_all_for_eig\"], \n",
    "            \"pred_horizons\": [16],\n",
    "            \"aggr_fns\": [\"min\"]\n",
    "        },\n",
    "        \"quantiles\": [0.95],\n",
    "        \"embedding\": {\n",
    "            \"embeddings\": [\"step_obs\"],\n",
    "            \"score_fns\": [\"top5_cosine\", \"top10_cosine\", \"mahal\", \"\"]\n",
    "        },\n",
    "        \"ensemble\": {\n",
    "            \"sample_sizes\": [128, 256],\n",
    "            \"pred_horizons\": [16],\n",
    "            \"action_spaces\": [\"traj\"]\n",
    "        },\n",
    "        \"entropy\": {\n",
    "            \"sample_sizes\": [128, 256],\n",
    "            \"pred_horizons\": [16],\n",
    "            \"error_fns\": [\"gmm_5\", \"kde_max_eig\", \"knn_5\"]\n",
    "        },\n",
    "        \"dummy_score\": {\n",
    "            \"constant\": 0.1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg = omegaconf.OmegaConf.create(conf_dict)\n",
    "\n",
    "demo_dataset_path = \"logs/datasets/no_domain_randomization_v8_simple_env\"\n",
    "test_dataset_path = \"logs/datasets/domain_randomization_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19106072",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = evaluate_dummy_score(cfg, demo_dataset_path, test_dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentinel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
