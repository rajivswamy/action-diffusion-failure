{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c38acc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### **Imports**\n",
    "# diffusion policy import\n",
    "from typing import Tuple, Sequence, Dict, Union, Optional, Callable\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import zarr\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from skvideo.io import vwrite\n",
    "from IPython.display import Video\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import glob\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd53efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General failure detector interface --> ideally implement this downstream\n",
    "class FailureDetector(ABC):\n",
    "    def __init__(self, calibration_dataset_dir: str, config: Dict):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def detect_episode(self, episode_dataset_dir) -> bool:\n",
    "        \"\"\"\n",
    "        Detects if a failure has occurred in the environment.\n",
    "        :param obs: The observation from the environment.\n",
    "        :return: True, detection time if a failure is detected, False, None otherwise.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def calibrate(self):\n",
    "        \"\"\"\n",
    "        Calibrates the failure detector.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74eb4c",
   "metadata": {},
   "source": [
    "## Episode Dataset Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973efa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union, List, Generator, Optional\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import IterableDataset\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def load_pickle(path: Union[str, pathlib.Path]) -> pd.DataFrame:\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "class EpisodeDataset(IterableDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: Union[str, pathlib.Path],\n",
    "        exec_horizon: int,\n",
    "        sample_history: int,\n",
    "        filter_success: bool = False,\n",
    "        filter_failure: bool = False,\n",
    "        filter_episodes: Optional[List[int]] = None,\n",
    "        max_episode_length: Optional[int] = None,\n",
    "        max_num_episodes: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Construct EpisodeDataset.\"\"\"\n",
    "        super().__init__()\n",
    "        assert exec_horizon >= 1 and sample_history >= 0\n",
    "        self._dataset_path = dataset_path\n",
    "        self._episode_files = sorted(glob.glob(os.path.join(dataset_path, \"*.pkl\")))\n",
    "        self._exec_horizon = exec_horizon\n",
    "        self._sample_history = sample_history\n",
    "        self._filter_success = filter_success\n",
    "        self._filter_failure = filter_failure\n",
    "        self._filter_episodes = filter_episodes\n",
    "        self._max_episode_length = max_episode_length\n",
    "        self._max_num_episodes = max_num_episodes\n",
    "\n",
    "    def __iter__(\n",
    "        self,\n",
    "    ) -> Generator[Union[Dict[str, Any], List[Dict[str, Any]]], None, None]:\n",
    "        \"\"\"Return sample.\"\"\"\n",
    "        num_episodes = 0\n",
    "        for i, file_path in enumerate(self._episode_files):\n",
    "            # if self._max_num_episodes is not None and num_episodes >= self._max_num_episodes:\n",
    "            if self._max_num_episodes is not None and i >= self._max_num_episodes:\n",
    "                continue\n",
    "\n",
    "            episode = load_pickle(file_path)\n",
    "            success = episode.iloc[0].to_dict().get(\"success\", True)\n",
    "            if (\n",
    "                (self._filter_success and success)\n",
    "                or (self._filter_failure and not success)\n",
    "                or (\n",
    "                    self._filter_episodes is not None\n",
    "                    and not isinstance(self._filter_episodes, str)\n",
    "                    and i in self._filter_episodes\n",
    "                )\n",
    "            ):\n",
    "                continue\n",
    "            else:\n",
    "                num_episodes += 1\n",
    "\n",
    "            for idx in range(\n",
    "                self._exec_horizon * self._sample_history,\n",
    "                len(episode), # rows of data in episode df\n",
    "                self._exec_horizon,\n",
    "            ):\n",
    "                if (\n",
    "                    self._max_episode_length is not None\n",
    "                    and episode.iloc[idx].to_dict()[\"timestep\"]\n",
    "                    >= self._max_episode_length\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                sample = [\n",
    "                    episode.iloc[j].to_dict()\n",
    "                    for j in range(\n",
    "                        idx - self._exec_horizon * self._sample_history,\n",
    "                        idx + 1,\n",
    "                        self._exec_horizon,\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "                # if len(sample) < 2:\n",
    "                #     continue\n",
    "                # assert all(x[\"episode\"] == i for x in sample), not relevant for our dataset\n",
    "                yield sample[0] if len(sample) == 1 else sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9f98acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = EpisodeDataset(\n",
    "    \"logs/datasets/no_domain_randomization_v5\",\n",
    "    exec_horizon=1,\n",
    "    sample_history=1,\n",
    "    filter_success=False,\n",
    "    filter_failure=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87378503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Completed iteration over dataset: 8654 samples seen.\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "try:\n",
    "    for sample in test_dataset:\n",
    "        # Optional sanity checks on each sample:\n",
    "        # assert isinstance(sample, dict) or list\n",
    "        # if list: assert all(\"timestep\" in s for s in sample)\n",
    "        assert len(sample) == 2\n",
    "        count += 1\n",
    "    print(f\"✅ Completed iteration over dataset: {count} samples seen.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error after {count} samples: {e!r}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a68c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(test_dataset)\n",
    "\n",
    "prev_data, cur_data = next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85e492",
   "metadata": {},
   "source": [
    "### Testing STAC Pipeline Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a7f99",
   "metadata": {},
   "source": [
    "error_fns in eval script:\n",
    "- \"mmd_rbf_all_median\"\n",
    "- \"kde_kl_all_for_eig\"\n",
    "- \"kde_kl_all_rev_eig\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b9c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Dict, Callable, Any, List, Optional\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import hydra\n",
    "import random\n",
    "import pickle\n",
    "import imageio\n",
    "import pathlib\n",
    "import omegaconf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.stac import utils\n",
    "import src.stac.dataset_utils as data_utils\n",
    "from src.stac import error_utils, metric_utils, action_utils\n",
    "\n",
    "import omegaconf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f8e260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSISTENCY_AGGR_FNS: Dict[str, Callable[[np.ndarray], float]] = {\n",
    "    \"min\": np.min,\n",
    "    \"max\": np.max,\n",
    "    \"mean\": np.mean,\n",
    "    \"std_dev\": np.std,\n",
    "    \"var\": np.var,\n",
    "}\n",
    "\n",
    "CONSISTENCY_ERROR_FNS: Dict[str, Dict[str, Any]] = {\n",
    "    \"mse_all\": {\n",
    "        \"error_fn\": \"mse\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "    },\n",
    "    \"mse_pos\": {\n",
    "        \"error_fn\": \"mse\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": True,\n",
    "    },\n",
    "    \"ate_pos\": {\n",
    "        \"error_fn\": \"ate\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "CONSISTENCY_DIST_ERROR_FNS = {\n",
    "    # MMD. - Maximum Mean Discrepancy (MMD)\n",
    "    \"mmd_rbf_pos\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": True,\n",
    "        \"gamma\": 1.0,\n",
    "    },\n",
    "    \"mmd_rbf_all\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": None,\n",
    "    },\n",
    "    \"mmd_rbf_all_1.0\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": 1.0,\n",
    "    },\n",
    "    \"mmd_rbf_all_median\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": \"median\",\n",
    "    },\n",
    "    \"mmd_rbf_all_eig\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": \"max_eig\",\n",
    "    },\n",
    "    \"mmd_rbf_all_0.1\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": 0.1,\n",
    "    },\n",
    "    \"mmd_rbf_all_0.5\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": 0.5,\n",
    "    },\n",
    "    \"mmd_rbf_all_5.0\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": 5.0,\n",
    "    },\n",
    "    \"mmd_rbf_all_10.0\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": 10.0,\n",
    "    },\n",
    "    \"mmd_rbf_all_100.0\": {\n",
    "        \"error_fn\": \"mmd_rbf\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"gamma\": 100.0,\n",
    "    },\n",
    "    # KDE For. KL.\n",
    "    \"kde_kl_all_for\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": 1.0,\n",
    "    },\n",
    "    \"kde_kl_all_for_eig\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": \"max_eig\",\n",
    "    },\n",
    "    \"kde_kl_all_for_0.1\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": 0.1,\n",
    "    },\n",
    "    \"kde_kl_all_for_0.5\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": 0.5,\n",
    "    },\n",
    "    \"kde_kl_all_for_5.0\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": 5.0,\n",
    "    },\n",
    "    \"kde_kl_all_for_10.0\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": 10.0,\n",
    "    },\n",
    "    \"kde_kl_all_for_100.0\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": True,\n",
    "        \"bandwidth\": 100.0,\n",
    "    },\n",
    "    # KDE Rev. KL.\n",
    "    \"kde_kl_all_rev\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": 1.0,\n",
    "    },\n",
    "    \"kde_kl_all_rev_eig\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": \"max_eig\",\n",
    "    },\n",
    "    \"kde_kl_all_rev_0.1\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": 0.1,\n",
    "    },\n",
    "    \"kde_kl_all_rev_0.5\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": 0.5,\n",
    "    },\n",
    "    \"kde_kl_all_rev_5.0\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": 5.0,\n",
    "    },\n",
    "    \"kde_kl_all_rev_10.0\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": 10.0,\n",
    "    },\n",
    "    \"kde_kl_all_rev_100.0\": {\n",
    "        \"error_fn\": \"kde_kl\",\n",
    "        \"ignore_gripper\": True,\n",
    "        \"ignore_rotation\": False,\n",
    "        \"forward\": False,\n",
    "        \"bandwidth\": 100.0,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Experiment keys.\n",
    "def temporal_consistency_exp_key(\n",
    "    pred_horizon: int,\n",
    "    sample_size: int,\n",
    "    error_fn: str,\n",
    "    aggr_fn: Optional[str] = None,\n",
    ") -> str:\n",
    "    if error_fn in CONSISTENCY_DIST_ERROR_FNS:\n",
    "        return (\n",
    "            f\"pred_horizon_{pred_horizon}_sample_size_{sample_size}_error_fn_{error_fn}\"\n",
    "        )\n",
    "    else:\n",
    "        return f\"pred_horizon_{pred_horizon}_sample_size_{sample_size}_error_fn_{error_fn}_aggr_fn_{aggr_fn}\"\n",
    "    \n",
    "def quantile_exp_key(exp_key: str, quantile: float = 0.95) -> str:\n",
    "    return f\"{exp_key}_quantile_{quantile}\"\n",
    "\n",
    "def get_consistency_aggr_fns(\n",
    "    cfg: omegaconf.DictConfig,\n",
    "    error_fn: str,\n",
    ") -> List[Optional[str]]:\n",
    "    if error_fn not in CONSISTENCY_ERROR_FNS:\n",
    "        return [None]\n",
    "    return cfg.eval.consistency.aggr_fns\n",
    "\n",
    "def compute_cum_scores(\n",
    "    results_frame: pd.DataFrame,\n",
    "    exp_keys: List[str],\n",
    ") -> pd.DataFrame:\n",
    "    for exp_key in exp_keys:\n",
    "        cum_scores = pd.Series(\n",
    "            data_utils.aggr_episode_key_data(\n",
    "                results_frame,\n",
    "                f\"{exp_key}_score\",\n",
    "                np.cumsum,\n",
    "            ),\n",
    "            name=f\"{exp_key}_cum_score\",\n",
    "        )\n",
    "        results_frame = pd.concat([results_frame, cum_scores], axis=1)\n",
    "\n",
    "    return results_frame\n",
    "\n",
    "# get RGB image from data\n",
    "def get_rgb(data: Dict[str, Any]) -> Optional[np.ndarray]:\n",
    "    return data['rgb'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3404cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temporal_consistency_errors(\n",
    "    cfg: omegaconf.DictConfig, dataset: EpisodeDataset\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute temporal consistency errors over dataset.\"\"\"\n",
    "    results = []\n",
    "    exp_keys = []\n",
    "\n",
    "    for prev_data, curr_data in iter(dataset):\n",
    "        assert curr_data[\"timestep\"] - prev_data[\"timestep\"] == cfg.model.ac_horizon\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"episode\": curr_data[\"episode\"],\n",
    "                \"timestep\": curr_data[\"timestep\"],\n",
    "                \"success\": curr_data.get(\"success\", True),\n",
    "            }\n",
    "        )\n",
    "        rgb = get_rgb(curr_data)\n",
    "        if isinstance(rgb, np.ndarray):\n",
    "            results[-1][\"rgb\"] = rgb\n",
    "\n",
    "        for sample_size in cfg.eval.consistency.sample_sizes:\n",
    "\n",
    "            # Subsample current and previous actions.\n",
    "            curr_actions = action_utils.subsample_actions(\n",
    "                curr_data[\"sampled_actions\"],\n",
    "                sample_size,\n",
    "            )\n",
    "            curr_skip_steps = curr_data.get(\"skip_steps\", None) # Should be None\n",
    "\n",
    "            prev_actions = action_utils.subsample_actions(\n",
    "                prev_data[\"sampled_actions\"],\n",
    "                sample_size,\n",
    "            )\n",
    "            prev_skip_steps = prev_data.get(\"skip_steps\", None) # Should be None\n",
    "\n",
    "            for error_fn in cfg.eval.consistency.error_fns:\n",
    "\n",
    "                if error_fn in CONSISTENCY_ERROR_FNS:\n",
    "                    prev_selected_actions = prev_data[\"executed_action\"]\n",
    "                    error_fn_kwargs = CONSISTENCY_ERROR_FNS[error_fn]\n",
    "                elif error_fn in CONSISTENCY_DIST_ERROR_FNS:\n",
    "                    prev_selected_actions = prev_actions\n",
    "                    error_fn_kwargs = CONSISTENCY_DIST_ERROR_FNS[error_fn]\n",
    "                else:\n",
    "                    raise ValueError(f\"Error function {error_fn} not supported.\")\n",
    "\n",
    "                for aggr_fn in get_consistency_aggr_fns(cfg, error_fn):\n",
    "                    for pred_horizon in cfg.eval.consistency.pred_horizons:\n",
    "                        if cfg.model.ac_horizon >= pred_horizon:\n",
    "                            continue\n",
    "\n",
    "                        exp_key = temporal_consistency_exp_key(\n",
    "                            pred_horizon=pred_horizon,\n",
    "                            sample_size=sample_size,\n",
    "                            error_fn=error_fn,\n",
    "                            aggr_fn=aggr_fn,\n",
    "                        )\n",
    "                        if exp_key not in exp_keys:\n",
    "                            exp_keys.append(exp_key)\n",
    "\n",
    "                        error = error_utils.compute_temporal_error(\n",
    "                            curr_action=curr_actions,\n",
    "                            prev_action=prev_selected_actions,\n",
    "                            pred_horizon=pred_horizon,\n",
    "                            exec_horizon=cfg.model.ac_horizon,\n",
    "                            sim_freq=cfg.env.args.freq,\n",
    "                            num_robots=cfg.env.num_eef,\n",
    "                            action_dim=cfg.env.dof,\n",
    "                            skip_steps=False,\n",
    "                            curr_skip_steps=curr_skip_steps,\n",
    "                            prev_skip_steps=prev_skip_steps,\n",
    "                            **error_fn_kwargs,\n",
    "                        )\n",
    "                        if error_fn in CONSISTENCY_ERROR_FNS:\n",
    "                            error = CONSISTENCY_AGGR_FNS[aggr_fn](error)\n",
    "                        results[-1][f\"{exp_key}_score\"] = error\n",
    "\n",
    "    results_frame = compute_cum_scores(pd.DataFrame(results), exp_keys)\n",
    "    return results_frame\n",
    "\n",
    "\n",
    "def evaluate_temporal_consistency(\n",
    "    cfg: omegaconf.DictConfig,\n",
    "    demo_dataset_path: Union[str, pathlib.Path],\n",
    "    test_dataset_path: Union[str, pathlib.Path],\n",
    ") -> Dict[str, Union[Dict[str, Any], pd.DataFrame]]:\n",
    "    \"\"\"Compute temporal consistency results.\"\"\"\n",
    "    # Construct episode iterable datasets.\n",
    "    demo_dataset = EpisodeDataset(\n",
    "        dataset_path=demo_dataset_path,\n",
    "        exec_horizon=1,\n",
    "        sample_history=1,\n",
    "        filter_success=getattr(cfg.eval, \"filter_demo_success\", False),\n",
    "        filter_failure=getattr(cfg.eval, \"filter_demo_failure\", True),\n",
    "        filter_episodes=getattr(cfg.eval, \"filter_demo_episodes\", None),\n",
    "        max_episode_length=getattr(cfg.eval, \"max_demo_episode_length\", None),\n",
    "        max_num_episodes=getattr(cfg.eval, \"max_num_episodes\", None),\n",
    "    )\n",
    "    test_dataset = EpisodeDataset(\n",
    "        dataset_path=test_dataset_path,\n",
    "        exec_horizon=1,\n",
    "        sample_history=1,\n",
    "        filter_success=getattr(cfg.eval, \"filter_test_success\", False),\n",
    "        filter_failure=getattr(cfg.eval, \"filter_test_failure\", False),\n",
    "        filter_episodes=getattr(cfg.eval, \"filter_test_episodes\", None),\n",
    "        max_episode_length=getattr(cfg.eval, \"max_test_episode_length\", None),\n",
    "        max_num_episodes=getattr(cfg.eval, \"max_num_episodes\", None),\n",
    "    )\n",
    "\n",
    "    # Compute scores for specified parameter sets.\n",
    "    results_dict = defaultdict(dict)\n",
    "    demo_results_frame = compute_temporal_consistency_errors(cfg, demo_dataset)\n",
    "    test_results_frame = compute_temporal_consistency_errors(cfg, test_dataset)\n",
    "\n",
    "    # Compute metrics for specified parameter sets.\n",
    "    for sample_size in cfg.eval.consistency.sample_sizes:\n",
    "        for error_fn in cfg.eval.consistency.error_fns:\n",
    "            for aggr_fn in get_consistency_aggr_fns(cfg, error_fn):\n",
    "                for pred_horizon in cfg.eval.consistency.pred_horizons:\n",
    "                    if cfg.model.ac_horizon >= pred_horizon:\n",
    "                        continue\n",
    "\n",
    "                    exp_key = temporal_consistency_exp_key(\n",
    "                        pred_horizon=pred_horizon,\n",
    "                        sample_size=sample_size,\n",
    "                        error_fn=error_fn,\n",
    "                        aggr_fn=aggr_fn,\n",
    "                    )\n",
    "\n",
    "                    for quantile in cfg.eval.quantiles:\n",
    "\n",
    "                        test_results_frame = metric_utils.compute_detection_results(\n",
    "                            exp_key=exp_key,\n",
    "                            quantile_key=quantile_exp_key(exp_key, quantile),\n",
    "                            results_dict=results_dict,\n",
    "                            demo_results_frame=demo_results_frame,\n",
    "                            test_results_frame=test_results_frame,\n",
    "                            detector=getattr(cfg.eval, \"detector\", \"quantile\"),\n",
    "                            detector_kwargs={\n",
    "                                \"quantile\": quantile,\n",
    "                                **getattr(cfg.eval, \"detector_kwargs\", {}),\n",
    "                            },\n",
    "                        )\n",
    "\n",
    "    return {\n",
    "        \"results_dict\": results_dict,\n",
    "        \"test_results_frame\": test_results_frame,\n",
    "        \"demo_results_frame\": demo_results_frame,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_dict = {\n",
    "    \"env\": {\n",
    "        \"args\": {\n",
    "            \"freq\": 1,\n",
    "            \"max_episode_length\": 300\n",
    "        },\n",
    "        \"dof\": 2,\n",
    "        \"num_eef\": 1\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"ac_horizon\": 8,\n",
    "    },\n",
    "    \"eval\": {\n",
    "        \"consistency\": {\n",
    "            \"sample_sizes\": [128, 256],\n",
    "            \"error_fns\": [\"mse_all\", \"mmd_rbf_all_median\", \"kde_kl_all_for_eig\"], \n",
    "            \"pred_horizons\": [16],\n",
    "            \"aggr_fns\": [\"min\"]\n",
    "        },\n",
    "        \"quantiles\": [0.95],\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg = omegaconf.OmegaConf.create(conf_dict)\n",
    "\n",
    "dataset_path = \"logs/datasets/no_domain_randomization_v5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be784a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode Results: ep_iid_cum | pred_horizon_16_sample_size_128_error_fn_mse_all_aggr_fn_min_quantile_0.95\n",
      "TPR: 0.46 | TNR: 0.65 | Acc: 0.55 | Bal. Acc: 0.55\n",
      "TP Time 108.78 (87.47)\n",
      "\n",
      "Episode Results: ep_iid_cum | pred_horizon_16_sample_size_128_error_fn_mmd_rbf_all_median_quantile_0.95\n",
      "TPR: 0.69 | TNR: 0.55 | Acc: 0.62 | Bal. Acc: 0.62\n",
      "TP Time 127.33 (83.51)\n",
      "\n",
      "Episode Results: ep_iid_cum | pred_horizon_16_sample_size_128_error_fn_kde_kl_all_for_eig_quantile_0.95\n",
      "TPR: 0.70 | TNR: 0.55 | Acc: 0.63 | Bal. Acc: 0.62\n",
      "TP Time 136.07 (84.02)\n"
     ]
    }
   ],
   "source": [
    "# Run eval\n",
    "output_data = evaluate_temporal_consistency(cfg, dataset_path, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dfcc49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentinel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
